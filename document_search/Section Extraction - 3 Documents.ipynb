{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proprietary-ceramic",
   "metadata": {},
   "source": [
    "# Section Extraction on Three Sample Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-glance",
   "metadata": {},
   "source": [
    "This notebook builds off the *Section Extraction - 1 Document* notebook to implement section extraction at scale on a small number (3) of sample documents.\n",
    "\n",
    "Use this repo's [Github Codespace](https://codespaces.new/CamDuffy1/rag_bootcamp) to run the notebook with all required dependencies installed.\n",
    "  - Specifically, the executable <code>[wkhtmltopdf](https://wkhtmltopdf.org/)</code> comes installed in the Github Codespace configuration. This is needed to use convert webpages to PDF documents using the <code>[pdfkit](https://pypi.org/project/pdfkit/)</code> library.\n",
    "  - These PDF documents can be too large (>25 GB) to store in the Github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8d6cd",
   "metadata": {},
   "source": [
    "#### Download S-1 PDF Documents and Create Truncated Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rising-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_url_dict = {\n",
    "    'reddit': 'https://www.sec.gov/Archives/edgar/data/1713445/000162828024006294/reddits-1q423.htm',\n",
    "    'facebook': 'https://www.sec.gov/Archives/edgar/data/1326801/000119312512034517/d287954ds1.htm',\n",
    "    'turo': 'https://www.sec.gov/Archives/edgar/data/1514587/000119312522005696/d145731ds1.htm',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "induced-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfkit\n",
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "output_dir = 'webpage_to_pdf'\n",
    "output_dir_path = os.path.join(current_path, output_dir)\n",
    "\n",
    "os.makedirs(output_dir_path) if not os.path.exists(output_dir_path) else None\n",
    "\n",
    "for key, url in s1_url_dict.items():\n",
    "    file_name = f\"{key}.pdf\"\n",
    "    output_file_path = os.path.join(output_dir_path, file_name)\n",
    "\n",
    "    pdfkit.from_url(url, output_file_path) if not os.path.exists(output_file_path) else print(f'File already exists at: {output_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "proper-universal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated file already exists at: /workspaces/rag_bootcamp/document_search/webpage_to_pdf/Truncated/reddit - Truncated - TOC.pdf\n",
      "Truncated file already exists at: /workspaces/rag_bootcamp/document_search/webpage_to_pdf/Truncated/turo - Truncated - TOC.pdf\n",
      "Truncated file already exists at: /workspaces/rag_bootcamp/document_search/webpage_to_pdf/Truncated/facebook - Truncated - TOC.pdf\n"
     ]
    }
   ],
   "source": [
    "# Truncate downloaded PDFs and add them to Truncated dir\n",
    "import fitz\n",
    "\n",
    "truncated_dir = os.path.join(output_dir_path, 'Truncated')                   # dir to save truncated files to\n",
    "os.makedirs(truncated_dir) if not os.path.exists(truncated_dir) else None    # create dir if not exists\n",
    "\n",
    "for item in os.listdir(output_dir_path):\n",
    "    item_path = os.path.join(output_dir_path, item)     # Construct the full path of the item\n",
    "    \n",
    "    # Check if the item is a file\n",
    "    if os.path.isfile(item_path) and item_path.lower().endswith('.pdf'):\n",
    "\n",
    "        filename = os.path.basename(item_path)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "\n",
    "        truncated_filename = f\"{filename} - Truncated - TOC{ext}\"\n",
    "        truncated_out_path = os.path.join(truncated_dir, truncated_filename)  # path to save the truncated file\n",
    "\n",
    "        if not os.path.exists(truncated_out_path):\n",
    "            source_doc = fitz.open(item_path)\n",
    "            truncated_doc = fitz.open()\n",
    "            truncated_doc.insert_pdf(source_doc, from_page=0, to_page=9)\n",
    "            truncated_doc.save(truncated_out_path)\n",
    "            print(f\"Saved truncated doc at: {truncated_out_path}\")\n",
    "            truncated_doc.close()\n",
    "            source_doc.close()\n",
    "        else:\n",
    "            print(f\"Truncated file already exists at: {truncated_out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86aa71",
   "metadata": {},
   "source": [
    "#### Extract Table of Contents from Truncated Documents and Generate List of Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "subsequent-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Cohere API key from environment variables in .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "dotenv_path = '/workspaces/rag_bootcamp/.env'\n",
    "_ = load_dotenv(dotenv_path=dotenv_path) if os.path.exists(dotenv_path) else print(f'No file found at: {dotenv_path}\\nPlease create a .env file and include your API keys.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "55f37be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8581/2083134847.py:16: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "# llama_index.llms.cohere does not support command-r model -- Use LiteLLM instead\n",
    "from llama_index.llms.litellm import LiteLLM\n",
    "llm = LiteLLM(\n",
    "    model=\"command-r\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "embed_model = CohereEmbedding(\n",
    "    cohere_api_key = os.environ['COHERE_API_KEY'],\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    ")\n",
    "\n",
    "from llama_index.core import ServiceContext\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    "    chunk_size=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d8207b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "import ast    # Built-in Abstract Syntax Trees module - Can convert a string contining a list to a list object\n",
    "\n",
    "sections_dict = {}  # instantiate dict to store lists of extracted sections from each document -- Format: {'company1': ['section1', 'section2', ...]}\n",
    "prompt = '''\n",
    "    Find the document's table of contents then list the sections in it as string elements in a Python list.\n",
    "    Your output should include nothing else. 'Table of Contents' should not be included as an item in the Python list.\n",
    "'''\n",
    "\n",
    "for item in os.listdir(truncated_dir):\n",
    "    truncated_path = os.path.join(truncated_dir, item)     # Construct the full path of the truncated file\n",
    "\n",
    "    filename = os.path.basename(truncated_path)\n",
    "    filename, ext = os.path.splitext(filename)\n",
    "    filename = filename.split(' ')[0]\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_files=[truncated_path])\n",
    "    documents = reader.load_data()    # get truncated document contents\n",
    "\n",
    "    truncated_index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=False)\n",
    "    \n",
    "    cohere_rerank = CohereRerank(\n",
    "        top_n=5\n",
    "    )\n",
    "    query_engine = truncated_index.as_query_engine(\n",
    "        node_postprocessors=[cohere_rerank],\n",
    "        similarity_top_k=8,\n",
    "    )\n",
    "    response = query_engine.query(prompt)\n",
    "    \n",
    "    document_sections = ast.literal_eval(response.response)\n",
    "    sections_dict[filename] = document_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c48b527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turo S-1 sections:\n",
      "    Prospectus summary\n",
      "    Risk factors\n",
      "    Special note regarding forward-looking statements\n",
      "    Market, industry, and other data\n",
      "    ...\n",
      "reddit S-1 sections:\n",
      "    Letter From Our Co-Founder\n",
      "    Prospectus Summary\n",
      "    Risk Factors\n",
      "    Special Note Regarding Forward-Looking Statements\n",
      "    ...\n",
      "facebook S-1 sections:\n",
      "    Prospectus Summary\n",
      "    Risk Factors\n",
      "    Special Note Regarding Forward-Looking Statements\n",
      "    Industry Data and User Metrics\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "# display sample output of PDF sections for each doc\n",
    "for key, value in sections_dict.items():\n",
    "    print(f\"{key} S-1 sections:\")\n",
    "    for i in range(4):\n",
    "        print(f\"    {value[i]}\")\n",
    "    print('    ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45705e1c",
   "metadata": {},
   "source": [
    "#### Get the Section Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4f040163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Risk Factors section from turo.pdf.\n",
      "    44778 words\n",
      "    Saved at: /workspaces/rag_bootcamp/document_search/webpage_to_pdf/extracted_sections/turo/Risk Factors.txt\n",
      "Extracted Risk Factors section from reddit.pdf.\n",
      "    44446 words\n",
      "    Saved at: /workspaces/rag_bootcamp/document_search/webpage_to_pdf/extracted_sections/reddit/Risk Factors.txt\n",
      "Extracted Risk Factors section from facebook.pdf.\n",
      "    14171 words\n",
      "    Saved at: /workspaces/rag_bootcamp/document_search/webpage_to_pdf/extracted_sections/facebook/Risk Factors.txt\n"
     ]
    }
   ],
   "source": [
    "from utils import get_item_after, get_words_from_PDF, get_section    # helper functions defined in utils.py\n",
    "\n",
    "target_section = 'Risk Factors'\n",
    "\n",
    "for key, value in sections_dict.items():\n",
    "    next_section = get_item_after(document_sections, target_section)\n",
    "\n",
    "    file_name = f\"{key}.pdf\"\n",
    "    file_path = os.path.join(output_dir_path, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File does not exist: {file_path}\")\n",
    "    else:\n",
    "        source_doc = fitz.open(file_path)\n",
    "        lowercase_words = get_words_from_PDF(doc=source_doc, start_page=15, lowercase=True)\n",
    "        source_doc.close()\n",
    "\n",
    "        start_str = target_section.lower()\n",
    "        end_str = next_section.lower()\n",
    "\n",
    "        body = get_section(\n",
    "            start_str=start_str,\n",
    "            end_str=end_str,\n",
    "            words=lowercase_words,\n",
    "        )\n",
    "\n",
    "        if body:\n",
    "            print(f\"Extracted {target_section} section from {file_name}.\\n{' '*4}{len(body.split(' '))} words\")\n",
    "            section_output_dir = os.path.join(output_dir_path, 'extracted_sections', key)\n",
    "            os.makedirs(section_output_dir) if not os.path.exists(section_output_dir) else None\n",
    "\n",
    "            output_file_name = f'{target_section}.txt'\n",
    "            section_output_file_path = os.path.join(section_output_dir, output_file_name)\n",
    "            \n",
    "            with open(section_output_file_path, \"w\") as file:\n",
    "                file.write(body)\n",
    "            print(f\"{' '*4}Saved at: {section_output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-money",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
