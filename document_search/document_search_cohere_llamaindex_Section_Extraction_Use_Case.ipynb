{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d86f6cd",
   "metadata": {},
   "source": [
    "# Cohere Document Search with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e6b6",
   "metadata": {},
   "source": [
    "This example shows how to use the Python [LlamaIndex](https://docs.llamaindex.ai/en/stable/) library to run a text-generation request against [Cohere's](https://cohere.com/) API, then augment that request using the text stored in a collection of local PDF documents.\n",
    "\n",
    "**Requirements:**\n",
    "- You will need an access key to Cohere's API key, which you can sign up for at (https://dashboard.cohere.com/welcome/login). A free trial account will suffice, but will be limited to a small number of requests.\n",
    "- After obtaining this key, store it in plain text in your home in directory in the `~/.cohere.key` file.\n",
    "- (Optional) Upload some pdf files into the `source_documents` subfolder under this notebook. We have already provided some sample pdfs, but feel free to replace these with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-cache",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ultimate-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-detection",
   "metadata": {},
   "source": [
    "Fresh installation of llama-index-core + integration packages -- New version of LlamaIndex introcudes breaking changes from the version on Vector Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mechanical-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall --quiet llama-index llama-index-core llama-index-llms-cohere llama_index.llms.litellm -y\n",
    "%pip install --quiet llama-index llama-index-core llama-index-llms-cohere llama_index.llms.litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-karen",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educational-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load source document\n",
    "import os\n",
    "import fitz # imports the pymupdf library\n",
    "\n",
    "source_doc_path = './S1_PDFs/Facebook S-1.pdf'\n",
    "source_doc = fitz.open(source_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acquired-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trunctate it to the first 10 pages -- This greatly improves speed & accuracy of the model retrieving the table of contents\n",
    "\n",
    "truncated_dir = './S1_PDFs/Truncated'                                        # dir to save truncated files to\n",
    "os.makedirs(truncated_dir) if not os.path.exists(truncated_dir) else None    # create dir if not exists\n",
    "\n",
    "truncated_doc = fitz.open()\n",
    "truncated_doc.insert_pdf(source_doc, from_page=0, to_page=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "governmental-aurora",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 pages - Original PDF doc\n",
      "10 pages - Truncated PDF doc\n",
      "Saved truncated doc at: ./S1_PDFs/Truncated/Facebook S-1 - Truncated - TOC.pdf\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.basename(source_doc_path)\n",
    "filename, ext = os.path.splitext(filename)\n",
    "\n",
    "truncated_filename = f\"{filename} - Truncated - TOC{ext}\"\n",
    "truncated_out_path = os.path.join(truncated_dir, truncated_filename)  # path to save the truncated file\n",
    "\n",
    "truncated_doc.save(truncated_out_path)\n",
    "\n",
    "print(f\"{len(source_doc)} pages - Original PDF doc\")\n",
    "print(f\"{len(truncated_doc)} pages - Truncated PDF doc\")\n",
    "print(f\"Saved truncated doc at: {truncated_out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-prison",
   "metadata": {},
   "source": [
    "### Extract the Table of Contents from Truncated Doc using Cohere Command-R Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "better-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cohere API Key\n",
    "import os\n",
    "from pathlib import Path\n",
    "try:\n",
    "    os.environ[\"COHERE_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "    os.environ[\"CO_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "except Exception:\n",
    "    print(f\"ERROR: You must have a Cohere API key available in your home directory at ~/.cohere.key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinguished-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(api_key=os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spread-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = co.chat( \n",
    "  model='command-r',\n",
    "  message='What are the Toronto Blue Jays?',\n",
    "  temperature=0.3,\n",
    "  chat_history=[],\n",
    "  prompt_truncation='AUTO',\n",
    "  stream=False,\n",
    "  citation_quality='accurate',\n",
    "  connectors=[],\n",
    "  documents=[]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-denial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "underlying-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "llm = Cohere(model=\"command\", api_key=os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dressed-opposition",
   "metadata": {},
   "outputs": [
    {
     "ename": "CohereAPIError",
     "evalue": "invalid request: model 'command-r' is not supported by the generate API",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCohereAPIError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m Cohere(\n\u001b[1;32m      4\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOHERE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommand-r\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the Toronto Blue Jays?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:294\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    279\u001b[0m     LLMCompletionStartEvent(\n\u001b[1;32m    280\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39m_self\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    285\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    286\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    287\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     },\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponseGen:\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/llama_index/llms/cohere/base.py:154\u001b[0m, in \u001b[0;36mCohere.complete\u001b[0;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_kwargs:\n\u001b[1;32m    149\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter `stream` is not supported by the `chat` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the `stream_chat` method instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 154\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletionResponse(\n\u001b[1;32m    163\u001b[0m     text\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m    164\u001b[0m     raw\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[1;32m    165\u001b[0m )\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/llama_index/llms/cohere/utils.py:70\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(client, max_retries, chat, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/pkgs/python-3.10.12/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/llama_index/llms/cohere/utils.py:68\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/cohere/client.py:221\u001b[0m, in \u001b[0;36mClient.generate\u001b[0;34m(self, prompt, prompt_vars, model, preset, num_generations, max_tokens, temperature, k, p, frequency_penalty, presence_penalty, end_sequences, stop_sequences, return_likelihoods, truncate, stream)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate endpoint.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mSee https://docs.cohere.ai/reference/generate for advanced arguments\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m        >>>     print(token)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m json_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    220\u001b[0m }\n\u001b[0;32m--> 221\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGENERATE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StreamingGenerations(response)\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/cohere/client.py:926\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, endpoint, json, files, method, stream, params)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mJSONDecodeError:  \u001b[38;5;66;03m# CohereAPIError will capture status\u001b[39;00m\n\u001b[1;32m    924\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError\u001b[38;5;241m.\u001b[39mfrom_response(response, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode json body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "File \u001b[0;32m/fs01/home/ws_cduffy/forked_repo/rag_bootcamp/envs/rag_dataloaders/rag_dataloaders_new/lib/python3.10/site-packages/cohere/client.py:868\u001b[0m, in \u001b[0;36mClient._check_response\u001b[0;34m(self, json_response, headers, status_code)\u001b[0m\n\u001b[1;32m    866\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-API-Warning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_response:  \u001b[38;5;66;03m# has errors\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    869\u001b[0m         message\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    870\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    871\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    872\u001b[0m     )\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    875\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected client error (status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    876\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    877\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    878\u001b[0m     )\n",
      "\u001b[0;31mCohereAPIError\u001b[0m: invalid request: model 'command-r' is not supported by the generate API"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.llms.cohere import Cohere\n",
    "llm = Cohere(\n",
    "    api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model='command-r'\n",
    ")\n",
    "query = \"What are the Toronto Blue Jays?\"\n",
    "result = llm.complete(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specific-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.litellm import LiteLLM\n",
    "llm = LiteLLM(\"command-r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the section after 'Risk Factors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-colonial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-highland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-conducting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-philippines",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-aside",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-miracle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-looking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-ceremony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e4da1f",
   "metadata": {},
   "source": [
    "## Set up the RAG workflow environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f637730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index import ServiceContext, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.cohereai import CohereEmbedding\n",
    "from llama_index.llms import Cohere\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ecf9ac",
   "metadata": {},
   "source": [
    "Set up some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edd103",
   "metadata": {},
   "source": [
    "Make sure other necessary items are in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b61e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.environ[\"COHERE_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "    os.environ[\"CO_API_KEY\"] = open(Path.home() / \".cohere.key\", \"r\").read().strip()\n",
    "except Exception:\n",
    "    print(f\"ERROR: You must have a Cohere API key available in your home directory at ~/.cohere.key\")\n",
    "\n",
    "# Look for the source-materials folder and make sure there is at least 1 pdf file here\n",
    "contains_pdf = False\n",
    "directory_path = \"./source_documents\"\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"ERROR: The {directory_path} subfolder must exist under this notebook\")\n",
    "for filename in os.listdir(directory_path):\n",
    "    contains_pdf = True if \".pdf\" in filename else contains_pdf\n",
    "if not contains_pdf:\n",
    "    print(f\"ERROR: The {directory_path} subfolder must contain at least one .pdf file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558afb",
   "metadata": {},
   "source": [
    "## Start with a basic generation request without RAG augmentation\n",
    "\n",
    "Let's start by asking the Cohere LLM a difficult, domain-specific question we don't expect it to have an answer to. A simple question like \"*What is the capital of France?*\" is not a good question here, because that's basic knowledge that we expect the LLM to know.\n",
    "\n",
    "Instead, we want to ask it a question that is very domain-specific that it won't know the answer to. A good example would an obscure detail buried deep within a company's annual report. For example:\n",
    "\n",
    "\"*How many Vector scholarships in AI were awarded in 2022?*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many Vector scholarships in AI were awarded in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a22c5",
   "metadata": {},
   "source": [
    "## Now send the query to Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00061d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "result = llm.complete(query)\n",
    "print(f\"Result: \\n\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c200",
   "metadata": {},
   "source": [
    "Without additional information, Cohere is unable to answer the question correctly. **Vector in fact awarded 109 AI scholarships in 2022.** Fortunately, we do have that information available in Vector's 2021-22 Annual Report, which is available in the `source_documents` folder. Let's see how we can use RAG to augment our question with a document search and get the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255ea68",
   "metadata": {},
   "source": [
    "## Ingestion: Load and store the documents from source-materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0304",
   "metadata": {},
   "source": [
    "Start by reading in all the PDF files from `source_documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdfs\n",
    "pdf_folder_path = \"./source_documents\"\n",
    "documents = SimpleDirectoryReader(pdf_folder_path).load_data()\n",
    "print(f\"Number of source materials: {len(documents)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7545e",
   "metadata": {},
   "source": [
    "## Define an embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc16fe",
   "metadata": {},
   "source": [
    "This embeddings model will convert the textual data from our PDF files into vector embeddings. These vector embeddings will later enable us to quickly find the chunk of text that most closely corresponds to our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = CohereEmbedding(\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_query\"\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    "    chunk_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1690e",
   "metadata": {},
   "source": [
    "## Storage: Store the documents in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ede5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008507b",
   "metadata": {},
   "source": [
    "## Retrieval: Now do a search to retrieve the chunk of document text that most closely matches our original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23499f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_retriever = index.as_retriever(service_context=service_context)\n",
    "search_query_retrieved_nodes = search_query_retriever.retrieve(query)\n",
    "print(f\"Search query retriever found {len(search_query_retrieved_nodes)} results\")\n",
    "print(f\"First result example:\\n{search_query_retrieved_nodes[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb632b45-b135-4561-9759-99fcc03e6959",
   "metadata": {},
   "source": [
    "That first result doesn't look right, but it's close? Could it be that we got the result that we wanted from that retrieval, but the results came back out of order? Let's try using a reranker to check which of our results is a closest match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea847fe",
   "metadata": {},
   "source": [
    "## Reranking: Improve the ordering of the document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CohereRerank()\n",
    "query_engine = index.as_query_engine(\n",
    "    node_postprocessors = [reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef217bc",
   "metadata": {},
   "source": [
    "## Final RAG-augmented query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63696ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = query_engine.query(query)\n",
    "print(f\"Result: {result}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_dataloaders_new",
   "language": "python",
   "name": "rag_dataloaders_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
